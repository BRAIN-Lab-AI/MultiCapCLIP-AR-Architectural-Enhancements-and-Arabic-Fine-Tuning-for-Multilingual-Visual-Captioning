# configs/adapt_bridge_keep512.yaml
clip_arch: ViT-B/16
clip_root: data/checkpoints
text_model: data/checkpoints/bert-base-uncased
decoder_config: configs/L6_H8_D512.json
init_with_bert: false

batch_size: 32
num_workers: 4
label_smoothing: 0.1
max_tokens: 40
optimizer: { opt: adamW, lr: 1e-4, weight_decay: 0.01, lr_mult: 1 }
schedular: { sched: fix, lr: 1e-4, epochs: 10, num_warmup_steps: 0.1 }

data_path: data/corpus/coco.txt
num_adapt_samples: -1

bridge:
  type: qformer_lite
  d_model: 512
  q_tokens: 24
  depth: 2
  heads: 4

freeze_decoder: true
freeze_visual_backbone: true

add_cross_attention: true
normalize: true
ConceptPromptPrefixer: true
n_concept_prompts: 16
concepts: true
concepts_path: data/concepts/coco/en/concepts.txt
with_related_caption_as_input: true
related_caption_topk: 5
noise_std: 0.1
