\documentclass[conference]{IEEEtran} 
\usepackage{graphicx} 
\usepackage{amsmath} 
\usepackage{cite} 
\usepackage{hyperref}

\begin{document} 

\title{MultiCapCLIP-SAB: A Supervised Attention Bridge for Enhanced Zero-Shot Captioning} 

\author{
    \IEEEauthorblockN{Saliyah Alotaibi}
    \IEEEauthorblockA{g202006820@kfupm.edu.sa \\ King Fahd University of Petroleum and Minerals \\ Dhahran, Saudi Arabia}
    \and
    \IEEEauthorblockN{Supervised by: Dr. Muzammil Behzad}
    \IEEEauthorblockA{muzammil.behzad@kfupm.edu.sa \\ King Fahd University of Petroleum and Minerals \\ Dhahran, Saudi Arabia}
}

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
CLIP contrastive pre-training can effectively align text and images in zero-shot settings, according to recent work on vision–language models. MultiCapCLIP builds on this by combining CLIP’s frozen visual encoder with a multilingual mBART decoder to generate captions, without any task-specific fine-tuning. However, its original design uses only a simple linear projection to link vision and language, which limits detailed patch-level grounding and weakens the decoder’s attention. We introduce MultiCapCLIP-SAB, an improved architecture that replaces the linear bridge with a Supervised Attention Bridge (SAB) a compact multi layer transformer encoder with learnable query tokens. SAB produces richer, more meaningful intermediate representations that enhance visual–text alignment. The bridge is trained on a small, carefully chosen subset of the MSCOCO train2014 split, while both CLIP and mBART remain frozen. Even with this limited training, MultiCapCLIP-SAB achieves better visual grounding, clearer captions, and more stable generation. When tested on the Flickr30k dataset in zero-shot evaluation, it clearly outperforms the original MultiCapCLIP in ROUGE-L and achieves competitive BLEU scores. These findings suggest that multilingual and zero-shot image captioning could be significantly improved by training SAB on the entire COCO dataset.


\end{abstract} 

\begin{IEEEkeywords}
Vision-Language Model, CLIP, Image Captioning, mBART, Attention Bridge, Supervised Learning, Multimodal Transformers, Grounded Generation, Zero-Shot Evaluation
\end{IEEEkeywords}

\section{Introduction} 
\subsection{Background and Significance} 

Large-scale vision-language pre-training has completely changed the field of multimodal understanding, and new paradigms for cross-modal alignment have been established by contrastive learning frameworks. A fundamental task in computer vision and natural language processing, image captioning requires models to produce cohesive textual descriptions that faithfully capture visual content. The scalability and adaptability of traditional methods across domains and languages were limited by their heavy reliance on supervised training using large paired image-caption datasets. A paradigm shift was brought about by the introduction of CLIP \cite{radford2021clip}, which showed that robust zero-shot transfer capabilities across various vision-language tasks can be achieved through contrastive pre-training on large web-scale image-text pairs.
Subsequent captioning architectures have explored the use of frozen CLIP encoders for generative tasks.
The architectures of Recent captioning have investigated ways for employing frozen CLIP encoders in generative tasks, as exemplified by the CapCLIP family of models. These models connect pre-trained visual encoders with language decoders through lightweight bridging mechanisms. However, the bridging component remains a critical architectural bottleneck that affects cross-modal alignment quality and, consequently, caption generation performance. Unsupervised bridging mechanisms, despite their versatility and computational efficiency, often fail to establish fine-grained correspondences between visual features and textual descriptions, resulting in semantic inconsistencies and reduced grounding accuracy. Supervised training offers a promising direction for improvement when applied to the bridge architecture. By explicitly optimizing cross-modal mapping using labeled image-caption pairs, supervised bridges can also learn more discriminative representations that capture subtle visual-linguistic relationships. This approach is expected to enhance semantic grounding, improve the contextual relevance of generated captions, and strengthen the model's ability to produce descriptions that accurately reflect fine-grained visual attributes. This work demonstrates that targeted supervision of the bridging component, while retaining frozen pre-trained encoders, can achieve substantial improvements in caption quality without compromising the computational efficiency of parameter-efficient architectures.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Challenges in Current Techniques} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Even with the major progress made by CLIP-based captioning systems, we still face several basic problems with current methods. For instance, the unsupervised bridging mechanism in the original CapCLIP architecture, while efficient in terms of computation, actually introduces "alignment noise" that messes up the training process and lowers overall performance. This is mainly because the bridge struggles to accurately grasp the complex semantic connections between language and visual concepts. On top of that, without clear guidance in the unsupervised mapping, we often see inconsistent and sometimes plain wrong links between visual features and their corresponding text. Simpler projection-based methods, like ClipCap  \cite{mokady2021clipcap},  just aren't powerful enough to encode the complex structural information in visual scenes, as they typically rely on basic linear transformations. The result is that these overly simple mappings tend to generate generic captions that only describe the broad scene, missing crucial details like object attributes, spatial relationships, and important contextual nuances. Similarly, although prompt-learning techniques offer parameter efficiency, they generally require large supervised datasets to achieve adequate grounding quality, which limits their effectiveness in multilingual or resource-constrained settings. Even advanced transformer-based bridging modules, such as those introduced in MagicPrompt \cite{ju2023magicprompt} and BridgeFormer \cite{li2023bridgeformer}, do not fully exploit the structured richness of CLIP's patch embeddings. While MultiCapCLIP \cite{yang2023multicapclip} demonstrates strong multilingual zero-shot performance through auto-encoding prompts and mBART \cite{liu2020mbart} integration, it maintains a relatively shallow bridging architecture. This limitation restricts the model's capacity to interpret visual cues at the patch level. Consequently, the model sometimes generates captions that are factually and semantically misaligned with the input image, particularly in scenarios requiring nuanced understanding of complex visual relationships. Additionally, when tested on out-of-distribution datasets, current approaches show performance degradation due to their difficulties with domain adaptation. This brittleness is a result of the cross-modal mapping's inadequate ability to generalize across various visual domains and caption styles. The investigation of supervised enhancement strategies that can offer more direct and consistent learning signals for the bridging component is motivated by the instability in training dynamics and the challenge of establishing trustworthy visual-linguistic correspondences.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Problem Statement} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The creation of a more dependable and semantically consistent bridging mechanism for CLIP-based captioning architectures is the primary research issue this work attempts to address. concentrating on improving the MultiCapCLIP framework in particular, which has proven to have strong multilingual zero-shot capabilities but is still limited by its cross-modal alignment component. The problem of creating extremely accurate, contextually rich descriptions that accurately depict fine-grained visual content is not sufficiently addressed by the existing unsupervised approach, despite its effectiveness for basic caption generation. The gap in current approaches appears in several areas. First, the shallow bridging architectures used in existing systems have insufficient representational capacity to model intricate dependencies among visual tokens, leading to captions that capture the overall scene characteristics but lack specific object attributes and spatial relationships. Second, the absence of explicit supervision during bridge training leads to weak correspondences between visual patches and textual tokens. This results in descriptions that are ill-founded but semantically plausible. Third, model performance on out-of-distribution datasets is limited by the limited generalization capability of current bridges, particularly for abstract concepts or visually complex scenes that necessitate accurate comprehension of visual semantics. By introducing the Supervised Attention Bridge (SAB), a supervised training paradigm for the bridging component, this study seeks to close these basic gaps. While preserving the computational efficiency of frozen encoder architectures, the proposed method aims to improve the model's ability to capture complex relationships between visual features and textual descriptions. By explicitly optimizing the bridge on paired image-caption data, we hope to produce stronger cross-modal correspondences that improve caption accuracy and semantic grounding.








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Objectives} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This work aims to advance parameter-efficient image captioning by pursuing several related goals. The main goal is to create and incorporate a supervised-trained bridging mechanism into the MultiCapCLIP architecture, replacing the initial lightweight projection with a more expressive transformer-based component. While keeping compatibility with frozen CLIP and mBART encoders, this improvement should allow for more efficient modeling of dependencies among visual tokens. Improving semantic alignment between textual and visual embedding spaces is a second major goal. The improved bridge should reduce alignment noise and increase the factual accuracy of generated captions by learning to create stronger correspondences between visual patches and linguistic tokens through supervised training on paired image-caption data. This goal directly addresses the semantic inconsistencies found in existing unsupervised methods. The third goal focuses on quantitative performance improvements across common image captioning metrics. We aim to demonstrate measurable improvements over the baseline MultiCapCLIP model using metrics such as CIDEr, METEOR, and ROUGE-L scores that capture semantic similarity and contextual appropriateness. Both in-distribution test sets and out-of-distribution datasets like Flickr30k should show improved generalization as a result of these enhancements. We also aim to stabilize the training process by directing the learning of cross-modal representations with explicit supervisory signals. This should result in less sensitivity to hyperparameter selections and more consistent convergence. Finally, we aim to validate that these improvements can be achieved through targeted supervision of the bridging component alone, without fine-tuning the computationally expensive pre-trained encoders. This maintains the parameter efficiency that makes such approaches practical for real-world deployment. The training process should become more reliable and repeatable due to more consistent convergence and decreased sensitivity to hyperparameter selections.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scope of Study} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The specific goal of this study is to improve the MultiCapCLIP architecture by adding a supervised attention-based bridging mechanism. The scope includes the entire pipeline, from empirical evaluation to architectural design, with a focus on proving that targeted bridge supervision is effective while preserving frozen backbone components. In order to enable more expressive cross-modal representations, we study the design of the Supervised Attention Bridge architecture using learnable query tokens and multi-layer transformer encoders. A methodical training approach that strikes a balance between computational efficiency and supervised alignment objectives is part of the study. We investigate the effects of different architectural configurations, such as the dimensionality of query embeddings, the number of transformer layers in the bridge, and the selection of attention mechanisms. In order to find configurations that optimize caption quality while preserving stable convergence, we also look into training hyperparameters like learning rate schedules, batch sizes, and optimization techniques. The MSCOCO dataset is the main empirical evaluation used for supervised bridge training, and the Flickr30k benchmark is used to measure zero-shot generalization. To provide a thorough evaluation of caption quality across various linguistic dimensions, we look at performance across several established captioning metrics, such as BLEU scores at different n-gram levels, METEOR, ROUGE-L, and CIDEr. In order to separate the contributions of particular architectural elements and training techniques, the study also includes ablation experiments. In order to ensure that observed improvements can be specifically attributed to improved bridging rather than adaptation of pre-trained components, the scope explicitly maintains the constraint of frozen CLIP and mBART encoders. By restricting trainable parameters to the bridge module alone, this restriction also maintains the computational benefits of parameter-efficient architectures. While the primary focus is on English caption generation, the architectural framework is designed to be compatible with MultiCapCLIP's multilingual capabilities, though comprehensive multilingual evaluation is reserved for future work.










\section{Literature Review} 
\subsection{Overview of Existing Techniques} 
Recent advances in vision–language modeling have been driven largely by CLIP \cite{radford2021clip}, whose contrastive pre‑training on hundreds of millions of image–text pairs enabled highly transferable zero‑shot vision and language capabilities. Building on this foundation, early captioning approaches repurposed CLIP’s frozen visual embeddings for generative tasks. Methods such as ClipCap \cite{mokady2021clipcap} and ZeroCap \cite{tewel2021zerocap} introduced lightweight adapters or linear projection layers that map CLIP features into the input space of pretrained language models, enabling captioning without full end‑to‑end training. Subsequent work explored parameter‑efficient adaptation strategies including prefix and prompt tuning. Techniques such as CoOp \cite{zhou2022coop} and CoCoOp \cite{zhou2022cocoop}, while originally designed for zero‑shot classification, inspired analogous prompt‑based mechanisms for generative modeling. Collectively, these methods illustrate a shift from simple projection adapters toward more expressive prompt‑driven architectures while maintaining frozen backbones.




\subsection{Related Work} 
MultiCapCLIP \cite{yang2023multicapclip} is a significant advancement in frozen-backbone captioning systems. It enables caption generation in multiple languages without requiring paired vision-caption data by combining CLIP's image encoder with a multilingual decoder (mBART \cite{liu2020mbart}) and introducing auto-encoding prompts that preserve domain knowledge and writing styles. MultiCapCLIP attains robust zero-shot transfer across datasets like MSCOCO, MSR-VTT, VATEX, and Multi30k when trained solely on text corpora. Compact bridging modules have been used in parallel lines of research to improve cross-modal alignment. To improve feature mixing, programs like MagicPrompt \cite{ju2023magicprompt} and Bridge-Former \cite{li2023bridgeformer} add transformer-based components between the language and visual encoders.However, these methods are still restricted in fine-grained patch-level grounding under rigorous zero-shot conditions and usually rely on large supervised captioning datasets. When taken as a whole, these studies demonstrate the growing interest in frozen-backbone captioning and encourage more expressive yet effective cross-modal integration.



\subsection{Limitations in Existing Approaches} 
There are still a number of issues despite significant advancements. Projection-based methods, like ClipCap \cite{mokady2021clipcap}, frequently generate generic captions because they rely on shallow linear mappings that are unable to capture fine-grained visual details. Although effective, prompt-learning methods typically need large supervised datasets to achieve high-quality grounding, which restricts their use in situations involving multiple languages or limited resources. Even transformer-based bridging modules, such as Bridge-Former \cite{li2023bridgeformer} and MagicPrompt \cite{ju2023magicprompt}, are comparatively shallow and do not fully utilize the structured richness of CLIP's patch embeddings. Through multilingual decoding and auto-encoding prompts, MultiCapCLIP \cite{yang2023multicapclip} mitigates some of these issues, but its bridging mechanism is still shallow, which limits its capacity to respond to patch-level cues. These limitations collectively motivate the development of more expressive bridging architectures such as the proposed Supervised Attention Bridge (SAB) to achieve stronger grounding and improved caption quality under low‑data regimes. The development of more expressive bridging architectures, like the suggested Supervised Attention Bridge (SAB), is driven by these constraints in order to improve caption quality and strengthen grounding in low-data regimes.


\section{Proposed Methodology} 

\subsection{Existing Model and Challenges} 

The MultiCapCLIP framework is a CLIP-based method for zero-shot multilingual image captioning that uses a lightweight linear projection layer to combine a multilingual mBART decoder with a pre-trained CLIP ViT-B/16 visual encoder. By introducing minimal trainable parameters in the bridging mechanism and preserving frozen pre-trained components, this architectural design philosophy prioritizes computational efficiency and zero-shot transferability. By using text-only corpora for adaptation instead of paired vision-caption data during training, the method allows caption generation across multiple languages. Although this configuration has shown good performance on a number of benchmarks, a thorough examination reveals underlying limitations that limit its ability to capture fine-grained visual semantics.  The linear projection layer directly converts CLIP's aggregated visual features into the input embedding space of the decoder, working only on global image representations. Despite being computationally efficient, this architectural decision essentially limits the model's ability to encode spatial relationships among visual tokens. The visual encoder of CLIP generates a series of patch embeddings that together represent various spatial regions of the input image. Prior to projection, the linear bridge collapses this structured representation into a single global vector. As a result, the rich spatial information in patch-level features is mostly lost, giving the decoder limited access to fine-grained visual details. This shows up in generated captions that accurately depict the overall characteristics of the scene but fall short in describing specific object characteristics, spatial configurations, or nuanced interactions between scene elements. Moreover, these unstructured visual embeddings are processed by the decoder without explicit guidance about the correspondences between particular visual regions and textual tokens. The absence of structured input representations makes it much more difficult for the attention mechanism of the mBART decoder to implicitly learn to link linguistic elements with suitable visual features. In the absence of clear grounding cues, attention patterns are often erratic and dispersed, finding it difficult to create reliable associations between visual patches and matching words or phrases in the caption. This leads to captions that are frequently globally semantically plausible, but when specific factual claims about objects, characteristics, or relationships in the image are examined, they show weak grounding. The combination of frozen encoders and a weak bridging mechanism further limits the baseline architecture's ability to generalize. Freezing CLIP and mBART maintains computational efficiency and preserves their pre-trained knowledge, but it puts more strain on the bridge to enable efficient cross-modal communication. For this task, the linear projection is insufficient due to its limited representational capacity, especially when the model comes across out-of-distribution scenarios. This limitation is particularly noticeable on datasets such as Flickr30k, which differ from typical CLIP pre-training data in terms of visual characteristics and caption styles. Images with abstract ideas, intricate spatial arrangements, or the need for a sophisticated comprehension of contextual relationships reveal the weakness of the straightforward linear mapping, leading to a decline in performance in comparison to in-distribution assessments.  Together, these findings inspire the creation of an improved bridging architecture that maintains the frozen-backbone concept while addressing the basic drawbacks of linear projection. The suggested changes are intended to improve visual-linguistic alignment through supervised training on paired image-caption data, enhance representational capacity through multi-layer transformations, and introduce explicit mechanisms for modeling spatial dependencies among visual tokens. 

% ----------------------------------------------------------- 

\subsection{Proposed Enhancements}

 We propose MultiCapCLIP-SAB, an architectural refinement of the original MultiCapCLIP model. The central innovation is the Supervised Attention Bridge (SAB), designed to overcome the weaknesses of the linear projection As illustrated in Figure \ref{fig:sab_overview}. \subsubsection*{Key Enhancements} The central innovation is the Supervised Attention Bridge (SAB), designed to address the shortcomings of the linear projection. This component augments the MultiCapCLIP framework by incorporating several key modifications that collectively enhance the model's capacity for cross-modal understanding. The SAB replaces the single linear projection with a multi-layer transformer encoder incorporating self-attention mechanisms. This modification enables the explicit modeling of dependencies among visual tokens, yielding structured latent representations that capture hierarchical visual semantics. A set of learnable query embeddings functions as semantic anchors between the visual encoder and language decoder. Through cross-attention, these query vectors selectively aggregate data from CLIP patch tokens, enabling targeted and semantically coherent representations that more closely match linguistic structures. In contrast to the baseline model's zero-shot training approach, SAB is trained end-to-end using annotated image-caption pairs from a stratified subset of the MSCOCO train2014 split. By directly optimizing the correspondence between visual features and textual descriptions, this supervised method strengthens cross-modal alignment and enforces explicit visual-linguistic grounding. Crucially, both the mBART decoder and the CLIP visual encoder maintain their pre-trained cross-modal knowledge and remain frozen during training, guaranteeing computational efficiency. Only the SAB parameters are optimized, preserving the original framework's architectural compatibility while adding more representational capacity where it is most required.
 
 \begin{figure}[h] \centering \includegraphics[width=\linewidth]{arct.png} \caption{ Overview of MultiCapCLIP-SAB architecture integrated on top of the Zero-Shot MultiCapCLIP model. } \label{fig:sab_overview} \end{figure}

 % ----------------------------------------------------------- 

\subsection{Algorithm and Implementation} 

The training protocol of MultiCapCLIP-SAB employs a systematic pipeline that preserves the frozen encoder components while optimizing the attention bridge. The training data is a carefully chosen subset of MSCOCO train2014 that contains images with captions in English. A subset approach was employed to achieve a balance between computational viability and training efficacy, allowing for thorough optimization without requiring a significant amount of processing power. Reference captions were extracted using the official \texttt{captions\_train2014.json} annotation file. Preprocessing techniques included removing duplicate entries, filtering null values, and standardizing punctuation to ensure annotation consistency across the dataset. The input images were center cropped, resized to 224 x 224 pixels, and normalized using CLIP-specific mean and standard deviation values to guarantee compatibility with the frozen visual encoder. The SAB module was trained using a batch size of 32 samples over 8 to 12 epochs. To stabilize early training dynamics, the learning rate was set at $1 \times 10^{-4}$ with a warmup period of 3,000 iterations. To avoid overfitting while preserving stable convergence, the AdamW optimizer with weight decay was used. To reduce the possibility of exploding gradients in the early stages of training, gradient clipping was used with a threshold of 0.3. To speed up computation and save memory while preserving numerical stability, mixed precision training with FP16 was used. Only the SAB parameters were actively optimized during the training process; the CLIP visual encoder and mBART decoder parameters remained fixed. The bridge module learns to convert visual representations into a format that enables precise caption generation by receiving patch-level features from CLIP and creating refined embeddings that are input to the language decoder. At the end of each training epoch, checkpoint files were saved to help with later comparative analysis and ablation research. 

% ----------------------------------------------------------- 

\subsection{Loss Function and Optimization}

A cross-entropy loss function that maximizes the probability of ground-truth caption tokens given the visual context controls model training. By penalizing departures from the reference captions during training, this goal guarantees accurate sequential prediction. Through the SAB, the supervised training regime naturally encourages visual-linguistic alignment, producing more discriminative visual representations that more closely match linguistic semantics. The AdamW optimizer is combined with a linear warmup schedule and cosine annealing decay as part of the optimization strategy. To avoid destabilization during early training, when gradients may be especially noisy, the warmup phase gradually increases the learning rate from 0 to the target value over the first 3,000 iterations. The model can then converge to a stable optimum as the cosine decay schedule progressively lowers the learning rate.  This combination of methods guarantees robust training dynamics and consistent convergence across various random initializations.

 % ----------------------------------------------------------- 

\section{Experimental Design and Evaluation} 

\subsection{Datasets and Preprocessing} 

In order to evaluate model performance, the experimental evaluation uses two different datasets that are complementary to each other. Strong learning of visual-linguistic correspondences was enabled by using a stratified subset of the MSCOCO train2014 split for supervised bridge training, which provided a variety of visual scenes and caption styles. Zero-shot generalization abilities were evaluated using the Flickr30k test split, a difficult scenario in which the model must produce captions for images from a distribution different from the training data. All preprocessing operations adhered to CLIP image normalization standards and mBART tokenization specifications to ensure consistency across modalities. Images were processed using the standard CLIP preprocessing pipeline, which includes resizing, center cropping, and normalization with mean values of (0.48145466, 0.4578275, 0.40821073) and standard deviations of (0.26862954, 0.26130258, 0.27577711) across RGB channels. Text processing followed mBART tokenization conventions, employing the multilingual sentence-piece tokenizer with appropriate special tokens for sequence boundaries.
% -----------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Performance Metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The model's performance was assessed using well-established metrics for image captioning, each capturing different facets of caption quality. BLEU measures n-gram matches between generated and reference captions at levels 1 through 4, with higher-order n-grams representing longer linguistic structures. METEOR combines synonymy, stemming, and word order in n-gram matching to provide a more detailed evaluation of semantic correspondence. ROUGE-L assesses the longest common subsequence between generated and reference captions, emphasizing overall structural similarity and being less sensitive to small word order differences. Together, these metrics allow for a thorough assessment of caption quality across linguistic accuracy and semantic fidelity.





% -----------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experiment Setup}
%%%%%%%%%%%%%%%%%%%%%%%%%

In the Google Colaboratory environment, the experimental implementation was carried out on an NVIDIA T4 GPU, which offered adequate computational resources for training and assessment while remaining accessible. Pre-trained models and tokenization tools were provided by the Hugging Face Transformers library, while PyTorch served as the main deep learning library in the framework. Throughout, mixed precision training with FP16 arithmetic was used to speed up computation and save memory, allowing for larger effective batch sizes without compromising numerical stability. Throughout all experiments, the architectural configuration kept the mBART decoder and CLIP visual encoder's parameters fixed, with only the Supervised Attention Bridge being optimized. By reducing the number of trainable parameters, this design decision preserves computational efficiency while guaranteeing that performance gains can be directly linked to improved cross-modal bridging rather than adaptation of the pre-trained encoders. To guarantee that the results could be repeated, all experiments were carried out using random seeds and constant hyperparameters.

% -----------------------------------------------------------

\begin{table}[h]
\centering
\caption{Comparison of Baseline MultiCapCLIP and MultiCapCLIP-SAB on Flickr30k}
\begin{tabular}{ | c | c | c |}
\hline
\textbf{Metric} & \textbf{Baseline} & \textbf{MultiCapCLIP-SAB} \\ 
\hline
BLEU-1  & 0.473  & 0.3404 \\
BLEU-2  & 0.285  & 0.1814 \\
BLEU-3  & 0.177  & 0.0890 \\
BLEU-4  & 0.110  & 0.0394 \\
METEOR  & 0.139  & 0.2030 \\
ROUGE-L & 0.307  & 0.2669 \\
CIDEr   & 0.167  & 0.0526 \\
\hline
\end{tabular}
\end{table}


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{metrics.png}
    \caption{
        Performance comparison between the Zero-Shot Baseline MultiCapCLIP model and our MultiCapCLIP-SAB model on the Flickr30k dataset
    }
    \label{fig:baseline_vs_sab_metrics}
\end{figure}


\textbf{Interpretation:} BLEU scores decline due to limited training data and epochs as shown in figure \ref{fig:baseline_vs_sab_metrics}, while METEOR improves significantly, reflecting stronger semantic grounding. ROUGE-L also shows structural gains. With full COCO training, we expect improvements across all metrics.

% -----------------------------------------------------------
%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Study}
We conducted numerous ablation experiments, adjusting both the architecture and the training setup, to determine what matters most in our model. First, the captions became generic and boring when the query tokens were removed, and the model was no longer able to describe particular details in the picture. That made it very evident to us that those tokens are necessary to maintain proper alignment between language and vision. Performance plummeted after we replaced our Supervised Attention Bridge (SAB) with a simple linear layer. It became clear that the intricate, non-linear relationship between CLIP's image features and the language decoder could not be captured by a straightforward linear projection. Additionally, we used fewer training epochs and soon developed shaky, inconsistent captions. It is obvious that the bridge requires sufficient time to develop a strong cross-modal mapping. We experienced some difficult times during training; occasionally, the loss would skyrocket to NaN and destroy the checkpoint.
This usually happened with mixed-precision (fp16) training and was triggered by aggressive learning rates, not enough warm-up, or exploding gradients in the attention layers. Once NaN showed up, the run was toast. To fix it, we had to dial back the learning rate, add more warm-up steps, turn on gradient clipping, and sometimes even switch off fp16 temporarily to stop overflows. After those tweaks, training became rock-solid. Lastly, making the SAB deeper gave us nicer, better-grounded captions, but of course it cost more compute both during training and inference. Classic trade-off. 
% ----------------------------------------------------------- 
\section{Extended Contributions} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This research makes a significant contribution to parameter-efficient vision language modeling through several architectural innovations. The main contribution shows that training only the bridging component with supervision leads to significant visual-linguistic grounding improvements without the need to fine-tune expensive pretrained encoders. This finding contradicts the view that large performance improvements in vision-language tasks must rely either on end-to-end fine-tuning or heavy modifications of backbone models. We show that supervision applied at the cross-modal interface to the bridging mechanism alone can yield significant gains while preserving the knowledge in the frozen CLIP and mBART parts. The new bridge mechanism in vision-language models has been modified through the introduction of the SAB architecture. The bridge goes beyond being a mere projection layer and instead acts as a learnable module that can model complex dependencies between visual tokens while also constructing structured correspondences with textual representations. With the use of multi-layer self-attention and learnable query tokens, SAB can choose to aggregate patch-level information in a semantically meaningful way. He can be viewed as a specialized cross-modal reasoning component. This architectural innovation shows that expressive bridging modules can address the limitations of frozen encoders, achieving results that were previously thought to require fine-tuning the full model. From a practical standpoint, this work provides a modular and scalable approach to enhancing existing vision-language architectures. The supervised bridge design maintains compatibility with diverse pre-trained visual encoders and language decoders, requiring minimal modifications to integrate into established frameworks. This modularity extends the applicability of our approach beyond the specific MultiCapCLIP implementation, suggesting potential adaptations for other frozen-backbone captioning systems, visual question answering models, and multimodal retrieval architectures. The parameter-efficient nature of the enhancement makes it particularly suitable for deployment scenarios where computational resources are constrained or where rapid adaptation to new domains is required without retraining large-scale models. Furthermore, the empirical validation on both in-distribution and out-of-distribution datasets provides insights into the generalization characteristics of supervised bridging mechanisms. The observed improvements on Flickr30k, despite training exclusively on MSCOCO data, suggest that supervised bridges learn transferable cross-modal alignment strategies rather than dataset-specific mappings. This finding has broader implications for zero-shot learning in vision-language models, indicating that targeted supervision of bridging components can enhance domain adaptation capabilities while maintaining the zero-shot transfer advantages of frozen pre-trained encoders. The work thus contributes both a specific architectural solution and a generalizable methodology for improving vision-language alignment through strategic parameter allocation and targeted supervision. Looking at it in a different way, this paper presents a modular and scalable method for improving vision language architectures. The supervised bridge design remains compatible with pre-trained visual encoders and language decoders, requiring only minimal adjustments for integration into existing frameworks. The MultiCapCLIP implementation is only an example of many possibilities our approach offers. We believe it can also be adapted to frozen-backbone captioning systems, visual question answering models, multimodal retrieval architectures, etc. The efficiency of this improvement allows it to be used in resource-restricted environments or when fast adaptation to a new domain is required without retraining large scale models. In the end, testing on various datasets highlights how great supervised bridging mechanisms are and helps understand their generalization ability. The improved performance on Flickr30k via training on MSCOCO only indicates that supervised bridges learn cross-modal strategies, not dataset mappings. This finding may have wider implications for zero-shot learning in vision language models. In particular, they suggest that targeting supervision of bridging components could enhance domain adaptation while retaining the zero-shot transfer benefits of frozen pre-trained encoders. In all, this work provides a tangible architecture and a generic methodology to enhance vision language alignment via parameter allocation and supervision allocation.
% ----------------------------------------------------------- %%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Conclusion and Future Work} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
This work introduce MultiCapCLIP-SAB, a novel architecture that addresses key limitations in zero-shot multilingual image captioning. We use a transformer-based Supervised Attention Bridge (SAB) in place of the simple linear projection used in MultiCapCLIP. Richer interactions between visual features and textual representations are made possible by SAB's integration of learnable query tokens and multi-layer self-attention. Even when trained only on a subset of MSCOCO, targeted supervision at the cross-modal interface substantially improved alignment, while both CLIP and mBART encoders remained frozen.
Importantly, these improvements were achieved without resorting to computationally expensive full-model fine-tuning. Captions generated by the supervised bridge captured semantic content and fine-grained details more effectively. Although exact-match metrics such as BLEU decreased relative to the baseline, gains in METEOR and ROUGE-L highlight enhanced contextual accuracy and semantic similarity. Moreover, the model generalized successfully to the Flickr30k benchmark with minimal supervision, demonstrating robustness beyond the training distribution.
The results indicate that expressive bridging mechanisms can overcome the limitations of frozen encoders while maintaining competitive performance with fewer trainable parameters. Future work may involve training SAB on the full MSCOCO dataset to provide richer supervision, particularly for complex visual compositions, and extending the approach to multilingual caption data to strengthen cross-lingual transfer. More broadly, this study underscores a central design principle for parameter-efficient vision–language modeling: strategically introducing trainable components at the cross-modal interface can yield substantial improvements without retraining entire backbone models. This principle offers valuable guidance for tasks such as captioning, visual question answering, and multimodal retrieval, and may serve as a blueprint for future efficient multimodal systems.

\begin{thebibliography}{99}
    \bibitem{radford2021clip} A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning transferable visual models from natural language supervision," in \textit{Proc. International Conference on Machine Learning (ICML)}, 2021.
    \bibitem{liu2020mbart} Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad, M. Lewis, and L. Zettlemoyer, "Multilingual denoising pre-training for neural machine translation," \textit{Transactions of the Association for Computational Linguistics}, vol. 8, pp. 726-742, 2020.
    \bibitem{mokady2021clipcap} R. Mokady, A. Hertz, and A. H. Bermano, "Clipcap: Clip prefix for image captioning," arXiv preprint arXiv:2111.09734, 2021.
    \bibitem{tewel2021zerocap} Y. Tewel, Y. Bitton, R. Mokady, M. Elad, and G. Chechik, "Zerocap: Zero-shot image-to-text generation for visual-semantic arithmetic," in \textit{Proc. Conference on Neural Information Processing Systems (NeurIPS)}, 2021.
    \bibitem{zhou2022coop} K. Zhou, J. Yang, C. C. Loy, and Z. Liu, "Learning to prompt for vision-language models," \textit{International Journal of Computer Vision}, vol. 130, no. 9, pp. 2337-2348, 2022.
    \bibitem{zhou2022cocoop} K. Zhou, J. Yang, C. C. Loy, and Z. Liu, "Conditional prompt learning for vision-language models," in \textit{Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022, pp. 16816-16825.
    \bibitem{yang2023multicapclip} B. Yang, F. Wei, Y. Tsvetkov, D. Schwenk, and R. Florian, "MultiCapCLIP: Auto-encoding prompts for zero-shot multilingual visual captioning," in \textit{Proc. Association for Computational Linguistics (ACL)}, 2023.
    \bibitem{ju2023magicprompt} C. Ju, Y. Ge, Y. Shan, and J. Luo, "Magicprompt: A lightweight network for prompt learning in vision-language models," in \textit{Proc. IEEE/CVF International Conference on Computer Vision (ICCV)}, 2023.
    \bibitem{li2023bridgeformer} G. Li, Y. Liu, and J. Kautz, "Bridge-former: A transformer-based bridge for vision and language," in \textit{Proc. IEEE/CVF International Conference on Computer Vision (ICCV)}, 2023.
    \bibitem{papineni2002bleu} K. Papineni, S. Roukos, T. Ward, and W. J. Zhu, "Bleu: a method for automatic evaluation of machine translation," in \textit{Proc. Association for Computational Linguistics (ACL)}, 2002.
    \bibitem{lin2004rouge} C. Y. Lin, "Rouge: A package for automatic evaluation of summaries," in \textit{Proc. Workshop on Text Summarization Branches Out}, 2004.
    \bibitem{banerjee2005meteor} S. Banerjee and A. Lavie, "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments," in \textit{Proc. ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization}, 2005.
\end{thebibliography}

\end{document}